{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d:\\\\Desktop\\\\Lipschitz_DSNN\\\\trial_notebooks',\n",
       " 'd:\\\\anaconda_neel\\\\python38.zip',\n",
       " 'd:\\\\anaconda_neel\\\\DLLs',\n",
       " 'd:\\\\anaconda_neel\\\\lib',\n",
       " 'd:\\\\anaconda_neel',\n",
       " '',\n",
       " 'd:\\\\anaconda_neel\\\\lib\\\\site-packages',\n",
       " 'd:\\\\anaconda_neel\\\\lib\\\\site-packages\\\\locket-0.2.1-py3.8.egg',\n",
       " 'd:\\\\anaconda_neel\\\\lib\\\\site-packages\\\\win32',\n",
       " 'd:\\\\anaconda_neel\\\\lib\\\\site-packages\\\\win32\\\\lib',\n",
       " 'd:\\\\anaconda_neel\\\\lib\\\\site-packages\\\\Pythonwin',\n",
       " 'd:\\\\anaconda_neel\\\\lib\\\\site-packages\\\\IPython\\\\extensions',\n",
       " 'C:\\\\Users\\\\NEELKANTH RAWAT\\\\.ipython']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('D:\\\\Desktop\\\\Lipschitz_DSNN') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I can start loading up the module and data and try to replicate some of the examples to understand the algorithm well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's load a saved configuration\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation_fn_params': {'activation_type': 'linearspline',\n",
       "  'groupsort_groupsize': 5,\n",
       "  'prelu_init': -1,\n",
       "  'lipschitz_constrained': True,\n",
       "  'spline_init': 'relu',\n",
       "  'spline_range': 0.5,\n",
       "  'spline_scaling_coeff': True,\n",
       "  'spline_size': 101,\n",
       "  'lmbda': 1e-07},\n",
       " 'dataset': {'function_type': 'f1',\n",
       "  'number_knots': 9,\n",
       "  'testing_dataset_size': 10000,\n",
       "  'training_dataset_size': 1000},\n",
       " 'exp_name': 'test',\n",
       " 'log_dir': '1d_exps/ortho',\n",
       " 'net_params': {'bias': True,\n",
       "  'layer_sizes': [1, 10, 10, 10, 1],\n",
       "  'projection': 'orthonormalize',\n",
       "  'weight_initialization': 'He_uniform'},\n",
       " 'optimizer': {'lr_spline_coeffs': 5e-05,\n",
       "  'lr_spline_scaling_coeffs': 0.0005,\n",
       "  'lr_weights': 0.002},\n",
       " 'seed': 5,\n",
       " 'training_options': {'batch_size': 10,\n",
       "  'epochs': 1000,\n",
       "  'nbr_models': 25,\n",
       "  'num_workers': 1}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the JSON file\n",
    "file_config_training=\"D:\\Desktop\\Lipschitz_DSNN\\configs\\config_1d.json\"\n",
    "with open(file_config_training, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BaseModel.py` and `simple_fc.py`: For Simple Fully Connected network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from architectures.simple_fc import SimpleFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from architectures.base_model import BaseModel\n",
    "from layers.lipschitzlinear import LipschitzLinear\n",
    "from projections.fc_projections import identity, bjorck_orthonormalize_fc\n",
    "\n",
    "\n",
    "class SimpleFCClassification(BaseModel):\n",
    "    \"\"\"simple architecture for a fully-connected network\"\"\"\n",
    "    def __init__(self, network_parameters, **params):\n",
    "        \n",
    "        super().__init__(**params)\n",
    "\n",
    "        modules = nn.ModuleList()\n",
    "\n",
    "        if network_parameters['projection'] == 'no_projection':\n",
    "            projection = identity\n",
    "        elif network_parameters['projection'] == 'orthonormalize':\n",
    "            if 'bjorck_iter' in network_parameters:\n",
    "                def proj(weights, lipschitz_goal):\n",
    "                    return bjorck_orthonormalize_fc(weights, lipschitz_goal,\n",
    "                                                    beta=0.5, iters=network_parameters['bjorck_iter'])\n",
    "                projection = proj\n",
    "            else:\n",
    "                projection = bjorck_orthonormalize_fc\n",
    "        else:\n",
    "            raise ValueError('Projection type is not valid')\n",
    "\n",
    "        layer_sizes = network_parameters['layer_sizes']\n",
    "\n",
    "\n",
    "        for i in range(len(layer_sizes)-2):\n",
    "            modules.append(LipschitzLinear(1, projection, layer_sizes[i], layer_sizes[i+1]))\n",
    "            modules.append(self.init_activation(('fc', layer_sizes[i+1])))\n",
    "\n",
    "\n",
    "        modules.append(LipschitzLinear(1, projection, layer_sizes[-2], layer_sizes[-1]))\n",
    "\n",
    "        self.initialization(init_type=network_parameters['weight_initialization'])\n",
    "        self.num_params = self.get_num_params()\n",
    "\n",
    "        self.layers = nn.Sequential(*modules)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" \"\"\"\n",
    "        ### apply layers and sigmoid function\n",
    "        x= self.layers(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Area Classification Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Area Classification Problem\n",
    "import numpy as np\n",
    "\n",
    "# Given function g (vectorized)\n",
    "g = lambda x: 0.4 * np.sin(-5*x)\n",
    "\n",
    "# Define the label function (vectorized)\n",
    "def S_shape_label_fn(coordinate_2d):\n",
    "    x1, x2 = coordinate_2d[:, 0], coordinate_2d[:, 1]\n",
    "    return np.logical_and(np.abs(x1 - g(x2)) <= 0.3, np.abs(x2) < 0.8).astype(int)\n",
    "\n",
    "# Generate random x_mat\n",
    "num_dp = 2000  # Assuming num_dp is defined\n",
    "x_mat = np.random.uniform(-1, 1, size=(num_dp, 2))\n",
    "\n",
    "# Create y_mat\n",
    "y_mat = S_shape_label_fn(x_mat)\n",
    "\n",
    "# Plotting\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.scatter(x_mat[y_mat == 1, 0], x_mat[y_mat == 1, 1], color='blue', label='Label 1')\n",
    "# plt.scatter(x_mat[y_mat == 0, 0], x_mat[y_mat == 0, 1], color='yellow', label='Label 0')\n",
    "# plt.title('2D Grid Plot of S_shape_label_fn')\n",
    "# plt.xlabel('X1')\n",
    "# plt.ylabel('X2')\n",
    "# plt.legend()\n",
    "# # plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "### later transfer it to the \n",
    "class CustomDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset class for our problem.\n",
    "    \"\"\"\n",
    "    def __init__(self, x_data, y_data):\n",
    "        self.x_data = x_data\n",
    "        self.y_data = y_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x_data[idx]\n",
    "        y = self.y_data[idx]\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils import tensorboard\n",
    "from architectures.simple_fc import SimpleFC\n",
    "from utils import metrics, utilities, spline_utils\n",
    "import matplotlib.pyplot as plt\n",
    "from layers.lipschitzlinear import LipschitzLinear\n",
    "from dataloader.Function_1D import (Function1D, generate_testing_set,\n",
    "    slope_1_ae, slope_1_flat, cosines, threshold)\n",
    "from activations.linearspline import LinearSpline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slope_normalization(cs, T):\n",
    "    lipschitz = torch.max(torch.abs(cs[:, 1:] - cs[:,:-1]), dim=1)[0]\n",
    "    new_cs = T * torch.div(cs.T, lipschitz).T\n",
    "\n",
    "    return new_cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing a general trainer function for our classification problem\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from torch.utils import tensorboard\n",
    "from utils import metrics, utilities\n",
    "\n",
    "class Trainer_classification:\n",
    "    \"\"\"\n",
    "    Trainer class for your classification problem.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, xdata, ydata, criterion, config, seed, device):\n",
    "        self.model = model\n",
    "        self.x = xdata\n",
    "        self.y = ydata\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        self.criterion = criterion \n",
    "\n",
    "        # Split dataset into train and validation sets\n",
    "        self.x_train, self.x_val, self.y_train, self.y_val = train_test_split(xdata, ydata,\n",
    "                                                                    test_size=config[\"training_options\"][\"validation_split\"], \n",
    "                                                                    random_state=seed)\n",
    "\n",
    "        # Prepare dataloaders \n",
    "        self.train_dataloader = DataLoader(CustomDataset(self.x_train, self.y_train), \n",
    "                                        batch_size=config[\"training_options\"][\"batch_size\"], \n",
    "                                        shuffle=True)\n",
    "        self.val_dataloader = DataLoader(CustomDataset(self.x_val, self.y_val), \n",
    "                                        batch_size=config[\"training_options\"][\"batch_size\"], \n",
    "                                        shuffle=False)\n",
    "\n",
    "        # Set up the optimizer \n",
    "        self.set_optimization()\n",
    "\n",
    "        # average train and val epoch loss\n",
    "        self.avg_train_loss_epoch=[]\n",
    "        self.avg_val_loss_epoch = []\n",
    "\n",
    "        # Stats to save about the models\n",
    "\n",
    "    ### setting up the optimizer\n",
    "    def set_optimization(self):\n",
    "        \"\"\" \"\"\"\n",
    "        #for i in range(self.nbr_models):\n",
    "        params_list = [{'params': spline_utils.get_no_spline_coefficients(self.model), \\\n",
    "                        'lr': self.config[\"optimizer\"][\"lr_weights\"]}]\n",
    "        if self.model.using_splines:\n",
    "            params_list.append({'params': spline_utils.get_spline_coefficients(self.model), \\\n",
    "                                'lr': self.config[\"optimizer\"][\"lr_spline_coeffs\"]})\n",
    "\n",
    "            if self.config[\"activation_fn_params\"][\"spline_scaling_coeff\"]:\n",
    "                params_list.append({'params': spline_utils.get_spline_scaling_coeffs(self.model), \\\n",
    "                                    'lr': self.config[\"optimizer\"][\"lr_spline_scaling_coeffs\"]})\n",
    "        self.optimizer = torch.optim.Adam(params_list)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Main training loop.\n",
    "        \"\"\"\n",
    "        for epoch in range(self.config[\"training_options\"][\"epochs\"]):\n",
    "            self.train_epoch(epoch)\n",
    "            self.validate_epoch(epoch)\n",
    "\n",
    "        # Need to add Additional post-training actions here\n",
    "        ### (right now I dont wanna save my checkpoints)\n",
    "\n",
    "    def train_epoch(self, epoch):\n",
    "        \"\"\"\n",
    "        Train the model for one epoch.\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        total_train_loss=0\n",
    "        tbar = tqdm(self.train_dataloader)\n",
    "        for batch_idx, (data, target) in enumerate(tbar): #(self.train_dataloader):\n",
    "            data, target = data.to(self.device), target.to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            output = self.model(data)### output shape: (N,1)\n",
    "            output = output.squeeze()\n",
    "            ### quantizing into predictions\n",
    "            # output_binary = torch.zeros_like(output)\n",
    "            # output_binary[output < 0.5] = 0\n",
    "            # output_binary[output >= 0.5] = 1    \n",
    "\n",
    "            ###\n",
    "            # print(f\"output is: {output}\")\n",
    "            # print(f\"target is: {target}\")\n",
    "            # Compute loss\n",
    "            data_loss = self.criterion(output, target.float())\n",
    "\n",
    "            # TV2 regulatisation\n",
    "            regularization=0\n",
    "            if self.model and self.config['activation_fn_params']['lmbda'] > 0:\n",
    "                regularization = self.config['activation_fn_params']['lmbda'] * self.model.TV2()\n",
    "            # total loss\n",
    "            total_loss = data_loss + regularization#data_loss #+ regularization\n",
    "            ### Yippie I figured out the issue. it is with the data_loss term \n",
    "\n",
    "            # Backward pass and optimization\n",
    "            self.optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Access and print gradients\n",
    "            # for name, param in self.model.named_parameters():\n",
    "            #     print(\"----------------checking the gradient evaluations----------------\")\n",
    "            #     if param.grad is not None:\n",
    "            #         print(f\"Gradient for {name}:\")\n",
    "            #         print(param.grad)\n",
    "            #     else:\n",
    "            #         print(f\"No gradient computed for {name}\")\n",
    "\n",
    "            total_train_loss+=total_loss\n",
    "\n",
    "            # Log training progress (optional)\n",
    "            # Your training progress logging code goes here\n",
    "            ### what should be the output of train_epoch?\n",
    "\n",
    "        # average training loss for the epoch\n",
    "        avg_train_loss = total_train_loss / len(self.train_dataloader)\n",
    "        print(f\"avg_train_loss: {avg_train_loss}\")\n",
    "        self.avg_train_loss_epoch.append(avg_train_loss)\n",
    "        \n",
    "\n",
    "    def validate_epoch(self, epoch):\n",
    "        \"\"\"\n",
    "        Validate the model for one epoch.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        total_val_loss=0\n",
    "        # Validation loop\n",
    "        with torch.no_grad():\n",
    "            for data, target in self.val_dataloader:\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                output = self.model(data)\n",
    "                output = output.squeeze()\n",
    "\n",
    "                ### quantizing into predictions\n",
    "                # output_binary = torch.zeros_like(output)\n",
    "                # output_binary[output < 0.5] = 0\n",
    "                # output_binary[output >= 0.5] = 1\n",
    "\n",
    "                # Compute validation metrics (e.g., accuracy, loss)\n",
    "                data_loss = self.criterion(output, target.float())\n",
    "                total_val_loss+=data_loss\n",
    "\n",
    "            avg_val_loss = total_val_loss / len(self.val_dataloader)\n",
    "            self.avg_val_loss_epoch.append(avg_val_loss)\n",
    "            # Additional validation actions go here\n",
    "        \n",
    "\n",
    "    def save_checkpoint(self, epoch):\n",
    "        \"\"\"\n",
    "        Save model checkpoint.\n",
    "        \"\"\"\n",
    "        # Your checkpoint saving code goes here\n",
    "\n",
    "    def load_checkpoint(self, checkpoint_path):\n",
    "        \"\"\"\n",
    "        Load model checkpoint.\n",
    "        \"\"\"\n",
    "        # Your checkpoint loading code goes here\n",
    "\n",
    "    # Additional methods for logging, evaluation, etc. go here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleFCClassification(\n",
      "  (layers): Sequential(\n",
      "    (0): LipschitzLinear(in_features=2, out_features=10, bias=True)\n",
      "    (1): LinearSpline(mode=fc, num_activations=10, init=relu, size=19, grid=0.100, lipschitz_constrained=True.)\n",
      "    (2): LipschitzLinear(in_features=10, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "### model parameters\n",
    "network_param = {'bias': True,\n",
    "    'layer_sizes': [2, 10, 1], ### we will consider 2-D Example later on\n",
    "    'projection': 'orthonormalize',\n",
    "    'weight_initialization': 'He_uniform'}\n",
    "\n",
    "additional_network_params= {'activation_type': 'linearspline', ### this is of interest\n",
    "    'groupsort_groupsize': 5,\n",
    "    'prelu_init': -1,\n",
    "    'lipschitz_constrained': True,### this is of interest\n",
    "    'spline_init': 'relu',### this is of interest\n",
    "    'spline_range': 0.9,### this is of interest [-1,1]\n",
    "    'spline_scaling_coeff': True,### this is of interest\n",
    "    'spline_size': 19,#101,###\n",
    "    'lmbda': 1e-07}### this is of interest [e-10,e2]\n",
    "\n",
    "for_optimizer = {'lr_spline_coeffs': 5e-05, # idk what exactly is this. Need to look into it well\n",
    "    'lr_spline_scaling_coeffs': 0.0005,\n",
    "    'lr_weights': 0.002}\n",
    "\n",
    "training_options = {'batch_size': 400,  \n",
    "    'epochs': 3,\n",
    "    'nbr_models': 1,\n",
    "    'num_workers': 1,\n",
    "    'validation_split':0.3}\n",
    "\n",
    "config_train_classification = { 'activation_fn_params':additional_network_params,\n",
    "                            'exp_name' : \"S shaped classification\",\n",
    "                            'net_params' : network_param,\n",
    "                            'optimizer' : for_optimizer,\n",
    "                            'training_options' : training_options\n",
    "}\n",
    "\n",
    "### defining the model\n",
    "ds_simple_fc_model= SimpleFCClassification(network_parameters=network_param,\n",
    "                             **additional_network_params)\n",
    "ds_simple_fc_model =ds_simple_fc_model.to('cpu')\n",
    "print(ds_simple_fc_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue I am facing at the moment: No gradients are being evaluated for the weights and biases. So we are never learning any parameters which could be the reason why the loss is not decreasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:34<00:00,  1.02it/s]\n",
      "  0%|          | 0/35 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss: 0.5450390577316284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:31<00:00,  1.11it/s]\n",
      "  0%|          | 0/35 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss: 0.5346651077270508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:33<00:00,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss: 0.5308161973953247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### let's club everything together\n",
    "import torch.nn as nn\n",
    "## data is : x_mat, y_mat\n",
    "## criterion is: criterion\n",
    "criterion= nn.BCELoss()\n",
    "### instantiate the Trainer \n",
    "trainer_classification = Trainer_classification(model=ds_simple_fc_model,\n",
    "                                                xdata=x_mat[:500,:],\n",
    "                                                ydata= y_mat[:500],\n",
    "                                                criterion=criterion,\n",
    "                                                config= config_train_classification, ## Need to add this\n",
    "                                                seed=12,\n",
    "                                                device='cpu'\n",
    "                                                )\n",
    "### Let's train the model\n",
    "trainer_classification.train() ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor(0.5450, grad_fn=<DivBackward0>),\n",
       "  tensor(0.5347, grad_fn=<DivBackward0>),\n",
       "  tensor(0.5308, grad_fn=<DivBackward0>)],\n",
       " [tensor(0.0472), tensor(0.0467), tensor(0.0463)])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss= trainer_classification.avg_train_loss_epoch\n",
    "val_loss = trainer_classification.avg_val_loss_epoch\n",
    "train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later I will work with the image data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torchvision\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# # Load CIFAR-10 dataset\n",
    "# transform = torchvision.transforms.Compose([\n",
    "#     torchvision.transforms.ToTensor()\n",
    "# ])\n",
    "\n",
    "# train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# # Create data loader\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# # Define classes\n",
    "# classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# # Function to show an image\n",
    "# def imshow(img):\n",
    "#     img = img / 2 + 0.5     # unnormalize\n",
    "#     npimg = img.numpy()\n",
    "#     plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "#     plt.show()\n",
    "\n",
    "# # Get some random training images\n",
    "# dataiter = iter(train_loader)\n",
    "# images, labels = dataiter.next()\n",
    "\n",
    "# # Show images\n",
    "# imshow(torchvision.utils.make_grid(images))\n",
    "# # Print labels\n",
    "# print(' '.join('%5s' % classes[labels[j]] for j in range(4)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
